# word2vec 原理

## 词向量

- One-hot representation(稀疏向量)：词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1，其他位置为0

  优点： 如果使用稀疏方式存储，非常简洁，实现时就可以用0,1,2,3,...来表示词语进行计算

  缺点：1.容易受维数灾难的困扰，尤其是将其用于 Deep Learning 的一些算法时;2.任何两个词都是孤立的，存在语义鸿沟词（任意两个词之间都是孤立的，不能体现词和词之间的关系）。

- Distributed representation（密集向量）：通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。

  优点：解决“词汇鸿沟”问题

  缺点：训练有难度,没有直接的模型可训练得到,所以采用通过训练语言模型的同时，得到词向量

## 语言模型

语言模型，就是指对自然语言进行假设和建模，使得能够用计算机能够理解的方式来表达自然语言。

语言模型形式化的描述就是给定一个T个词的字符串s，看它是自然语言的概率P(w1,w2,…,wt)。w1 到 wT 依次表示这句话中的各个词。有个很简单的推论是：

$$
  {\rm{p}}\left( s \right) = {\rm{p}}\left( {{w_1},{w_2}, \cdots {w_T}} \right)   = {\rm{p}}\left( {{w_1}} \right){\rm{p(}}{w_2}{\rm{|}}{w_1}){\rm{p}}({w_3}|{w_1},{w_2}) \cdots {\rm{p}}({w_t}|{w_1},{w_2}, \cdots {w_{T - 1}})
$$

上面那个概率表示的意义是：第一个词确定后，看后面的词在前面的词出现的情况下出现的概率。如一句话“大家喜欢吃苹果”，总共四个词“大家”，“喜欢”，“吃”，“苹果”，怎么分词现在不讨论，总之词已经分好，就这四个。那么这句话是一个自然语言的概率是：

P(大家，喜欢，吃，苹果)=p(大家)p(喜欢|大家)p(吃|大家,喜欢)p(苹果|大家,喜欢,吃)

p(大家)表示“大家”这个词在语料库里面出现的概率；

p(喜欢|大家)表示“喜欢”这个词出现在“大家”后面的概率;

p(吃|大家，喜欢)表示“吃”这个词出现在“大家喜欢”后面的概率；

p(苹果|大家,喜欢,吃)表示“苹果”这个词出现在“大家喜欢吃”后面的概率。

把这些概率连乘起来，得到的就是这句话平时出现的概率。

如果这个概率特别低，说明这句话不常出现，那么就不算是一句自然语言，因为在语料库里面很少出现。如果出现的概率高，就说明是一句自然语言。

看到了上面的计算，看有多麻烦：只有四个词的一句话，需要计算的是p(大家)，p(喜欢|大家)，p(吃|大家,喜欢)，p(苹果|大家,喜欢,吃)这四个概率，这四个概率还要预先计算好，考虑词的数量，成千上万个，再考虑组合数，p(吃|大家,喜欢)这个有“大家”、“喜欢”和“吃”的组合，总共会上亿种情况吧；再考虑p(苹果|大家,喜欢,吃)这个概率，总共也会超过万亿种。

从上面的情况看来，计算起来是非常麻烦的，一般都用偷懒的方式。

为了表示简单，上面的公式用下面的方式表示

$$
{\rm{p}}\left( {\rm{s}} \right) = {\rm{p}}\left( {{w_1},{w_2}, \cdots {w_T}} \right) =  \prod \limits_{i = 1}^T p({w_i}|Contex{t_i})
$$

其中，如果Contexti是空的话，就是它自己p(w)，另外如“吃”的Context就是“大家”、“喜欢”，其余的对号入座。

### N-gram模型

接下来说怎么计算$p({w_i}|Contex{t_i})$，上面看的是跟据这句话前面的所有词来计算，那么就得计算很多了，比如就得把语料库里面p(苹果|大家,喜欢,吃)这种情况全部统计一遍，那么为了计算这句话的概率，就上面那个例子，都得扫描四次语料库。这样一句话有多少个词就得扫描多少趟，语料库一般都比较大，越大的语料库越能提供准确的判断。这样的计算速度在真正使用的时候是万万不可接受的，线上扫描一篇文章是不是一推乱七八糟的没有序列的文字都得扫描很久，这样的应用根本没人考虑。

最好的办法就是直接把所有的$p({w_i}|Contex{t_i})$提前算好了，那么根据排列组上面的来算，对于一个只有四个词的语料库，总共就有4!+3!+2!+1!个情况要计算，那就是24个情况要计算；换成1000个词的语料库，就是$\sum \limits_{i = 1}^{1000} i!$个情况需要统计，对于计算机来说，计算这些东西简直是开玩笑。

这就诞生了很多偷懒的方法，N-gram模型是其中之一了。N-gram什么情况呢？上面的context都是这句话中这个词前面的所有词作为条件的概率，N-gram就是只管这个词前面的n-1个词，加上它自己，总共n个词，计算$p({w_i}|Contex{t_i})$只考虑用这n个词来算，换成数学的公式来表示，就是

${\rm{p}}\left( {{w_i}{\rm{|}}Contex{t_i}} \right) = {\rm{p}}({w_i}|{w_{i - n + 1}},{w_{i - n + 2}}, \cdots ,{w_{i - 1}})$
这里如果n取得比较小的话，就比较省事了，当然也要看到n取得太小，会特别影响效果的，有可能计算出来的那个概率很不准。怎么平衡这个效果和计算就是大牛们的事情了，据大牛们的核算，n取2效果都还凑合，n取3就相当不错了，n取4就顶不住了。看下面的一些数据，假设词表中词的个数 |V| = 20,000 词，那么有下面的一些数据。

  ![N-Gram](./images/n-gram.jpg)

照图中的数据看去，取n=3是目前计算能力的上限了。在实践中用的最多的就是bigram和trigram了，而且效果也基本够了。

N-gram模型也会有写问题，总结如下：

1、n不能取太大，取大了语料库经常不足，所以基本是用降级的方法

2、无法建模出词之间的相似度，就是有两个词经常出现在同一个context后面，但是模型是没法体现这个相似性的。

3、有些n元组（n个词的组合，跟顺序有关的）在语料库里面没有出现过，对应出来的条件概率就是0，这样一整句话的概率都是0了，这是不对的，解决的方法主要是两种：平滑法（基本上是分子分母都加一个数）和回退法（利用n-1的元组的概率去代替n元组的概率）

### N-pos模型
当然学术是无止境的，有些大牛觉得这还不行，因为第i个词很多情况下是条件依赖于它前面的词的语法功能的，所以又弄出来一个n-pos模型，n-pos模型也是用来计算的，但是有所改变，先对词按照词性(Part-of-Speech，POS)进行了分类，具体的数学表达是

${\rm{p}}\left( {{w_i}{\rm{|}}Contex{t_i}} \right) = {\rm{p}}\left( {{w_i}|{\rm{c}}\left( {{w_{i - n + 1}}} \right),{\rm{c}}\left( {{w_{i - n + 2}}} \right), \cdots ,{\rm{c}}\left( {{w_{i - 1}}} \right)} \right)$
其中c是类别映射函数，功能是把V个词映射到K个类别（1=<K<=V）。这样搞的话，原来的V个词本来有{V^n}种n元组减少到了${\rm{V}} \times {{\rm{K}}^{n - 1}}$种。

其他的模型还很多，不一一介绍了。

### CBOW与Skip-Gram用于神经网络语言模型

word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。

- CBOW(Continuous Bag-Of-Words):输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量,CBOW模型能够根据输入周围n-1个词来预测出这个词本身,CBOW模型的输入是某个词A周围的n个单词的词向量之和，输出是词A本身的词向量
- Skip-Gram:输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量,skip-gram模型能够根据词本身来预测周围有哪些词,skip-gram模型的输入是词A本身，输出是词A周围的n个单词的词向量

## word2vec基础

### 霍夫曼树

word2vec 是google在2013年推出的一个NLP工具，它的特点是将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系，CBOW与Skip-Gram来训练模型与得到词向量，但是并没有使用传统的DNN模型。最先优化使用的数据结构是用霍夫曼树来代替隐藏层和输出层的神经元，霍夫曼树的叶子节点起到输出层神经元的作用，叶子节点的个数即为词汇表的小大。 而内部节点则起到隐藏层神经元的作用。

霍夫曼树的建立其实并不难，过程如下：

- 输入：权值为(w1,w2,...wn)的n个节点
- 输出：对应的霍夫曼树
  - 将(w1,w2,...wn)看做是有n棵树的森林，每个树仅有一个节点。
  - 在森林中选择根节点权值最小的两棵树进行合并，得到一个新的树，这两颗树分布作为新树的左右子树。新树的根节点权重为左右子树的根节点权重之和。
  - 将之前的根节点权值最小的两棵树从森林删除，并把新树加入森林。
  - 重复,直到森林里只有一棵树为止。

一般得到霍夫曼树后我们会对叶子节点进行霍夫曼编码，由于权重高的叶子节点越靠近根节点，而权重低的叶子节点会远离根节点，这样我们的高权重节点编码值较短，而低权重值编码值较长。这保证的树的带权路径最短，也符合我们的信息论，即我们希望越常用的词拥有更短的编码。在word2vec中，约定左子树编码为1，右子树编码为0，同时约定左子树的权重不小于右子树的权重。

### 基于Hierarchical Softmax的模型

理论上说，无论是CBOW模型还是skip-gram模型，其具体的实现都可以用神经网络来完成。问题在于，这样做的计算量太大了。我们可以简略估计一下。首先定义一些变量的含义：

- n : 一个词的上下文包含的词数，与n-gram中n的含义相同
- m : 词向量的长度，通常在10~100
- h : 隐藏层的规模，一般在100量级
- N ：词典的规模，通常在1W~10W
- T : 训练文本中单词个数

以CBOW为例，输入层为n-1个单词的词向量，长度为m(n-1)，隐藏层的规模为h,输出层的规模为N，那么前向的时间复杂度就是o(m(n-1)h+hN) = o(hN)。
这还是处理一个词所需要的复杂度。如果要处理所有文本，则需要o(hNT)的时间复杂度。这个是不可接受的。同时我们也注意到，o(hNT)之中，h和T的值相对固定，想要对其进行优化，主要还是应该从N入手。而输出层的规模之所以为N，是因为这个神经网络要完成的是N选1的任务。那么可不可以减小N的值呢？答案是可以的。解决的思路就是将一次分类分解为多次分类，这也是Hierarchical Softmax的核心思想。
举个栗子，有[1,2,3,4,5,6,7,8]这8个分类，想要判断词A属于哪个分类，我们可以一步步来，首先判断A是属于[1,2,3,4]还是属于[5,6,7,8]。如果判断出属于[1,2,3,4]，那么就进一步分析是属于[1,2]还是[3,4]，以此类推，如图中所示的那样。这样一来，就把单个词的时间复杂度从o(h*N)降为o(h*logN)，更重要的减少了内存的开销。

传统的神经网络词向量语言模型，里面一般有三层，输入层（词向量），隐藏层和输出层（softmax层）。里面最大的问题在于从隐藏层到输出的softmax层的计算量很大，因为要计算所有词的softmax概率，再去找概率最大的值。

word2vec对这个模型做了改进，首先，对于从输入层到隐藏层的映射，采用简单的对所有输入词向量求和并取平均的方法。比如输入的是三个4维词向量：(1,2,3,4),(9,6,11,8),(5,10,7,12),那么我们word2vec映射后的词向量就是(5,6,7,8)。由于这里是从多个词向量变成了一个词向量。从隐藏层到输出的softmax层这里的计算,word2vec采用了霍夫曼树来代替从隐藏层到输出softmax层的映射，神经网络语言模型相比，我们的霍夫曼树的所有内部节点就类似之前神经网络隐藏层的神经元,其中，根节点的词向量对应我们的投影后的词向量，而所有叶子节点就类似于之前神经网络softmax输出层的神经元，叶子节点的个数就是词汇表的大小。在霍夫曼树中，隐藏层到输出层的softmax映射不是一下子完成的，而是沿着霍夫曼树一步步完成的，因此这种softmax取名为"Hierarchical Softmax"。

### word2vec的大概流程

- 分词 / 词干提取和词形还原。 中文和英文的nlp各有各的难点，中文的难点在于需要进行分词，将一个个句子分解成一个单词数组。而英文虽然不需要分词，但是要处理各种各样的时态，所以要进行词干提取和词形还原。
- 构造词典，统计词频。这一步需要遍历一遍所有文本，找出所有出现过的词，并统计各词的出现频率。
- 构造树形结构。依照出现概率构造Huffman树。如果是完全二叉树，则简单很多，后面会仔细解释。需要注意的是，所有分类都应该处于叶节点，像下图显示的那样

  ![Huffman-tree-1](./images/huffman-tree-1.jpg)

- 生成节点所在的二进制码。拿上图举例，22对应的二进制码为00,而17对应的是100。也就是说，这个二进制码反映了节点在树中的位置，就像门牌号一样，能按照编码从根节点一步步找到对应的叶节点。
- 初始化各非叶节点的中间向量和叶节点中的词向量。树中的各个节点，都存储着一个长为m的向量，但叶节点和非叶结点中的向量的含义不同。叶节点中存储的是各词的词向量，是作为神经网络的输入的。而非叶结点中存储的是中间向量，对应于神经网络中隐含层的参数，与输入一起决定分类结果。
- 训练中间向量和词向量。对于CBOW模型，首先将词A附近的n-1个词的词向量相加作为系统的输入，并且按照词A在步骤4中生成的二进制码，一步步的进行分类并按照分类结果训练中间向量和词向量。举个栗子，对于绿17节点，我们已经知道其二进制码是100。那么在第一个中间节点应该将对应的输入分类到右边。如果分类到左边，则表明分类错误，需要对向量进行修正。第二个，第三个节点也是这样，以此类推，直到达到叶节点。因此对于单个单词来说，最多只会改动其路径上的节点的中间向量，而不会改动其他节点。

  ![Huffman-tree-2](./images/huffman-tree-2.jpg)